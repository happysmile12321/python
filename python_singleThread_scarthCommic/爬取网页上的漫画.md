###### 网页地址

[网页网址](<https://www.mh1234.com/>)

---

###### python爬的错误代码

```python
Traceback (most recent call last):
  File "C:\soft\python\lib\site-packages\urllib3\connectionpool.py", line 603, in urlopen
    chunked=chunked)
  File "C:\soft\python\lib\site-packages\urllib3\connectionpool.py", line 344, in _make_request
    self._validate_conn(conn)
  File "C:\soft\python\lib\site-packages\urllib3\connectionpool.py", line 843, in _validate_conn
    conn.connect()
  File "C:\soft\python\lib\site-packages\urllib3\connection.py", line 370, in connect
    ssl_context=context)
  File "C:\soft\python\lib\site-packages\urllib3\util\ssl_.py", line 355, in ssl_wrap_socket
    return context.wrap_socket(sock, server_hostname=server_hostname)
  File "C:\soft\python\lib\ssl.py", line 423, in wrap_socket
    session=session
  File "C:\soft\python\lib\ssl.py", line 870, in _create
    self.do_handshake()
  File "C:\soft\python\lib\ssl.py", line 1139, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1076)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\soft\python\lib\site-packages\requests\adapters.py", line 449, in send
    timeout=timeout
  File "C:\soft\python\lib\site-packages\urllib3\connectionpool.py", line 641, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\soft\python\lib\site-packages\urllib3\util\retry.py", line 399, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.mh1234.com', port=443): Max retries exceeded with url: /comic/9329/559074.html?p=3 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1076)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:/resource/Desktop/sf.py", line 6, in <module>
    resp = requests.get("https://www.mh1234.com/comic/9329/559074.html?p=3", headers=headers)
  File "C:\soft\python\lib\site-packages\requests\api.py", line 75, in get
    return request('get', url, params=params, **kwargs)
  File "C:\soft\python\lib\site-packages\requests\api.py", line 60, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\soft\python\lib\site-packages\requests\sessions.py", line 533, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\soft\python\lib\site-packages\requests\sessions.py", line 646, in send
    r = adapter.send(request, **kwargs)
  File "C:\soft\python\lib\site-packages\requests\adapters.py", line 514, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='www.mh1234.com', port=443): Max retries exceeded with url: /comic/9329/559074.html?p=3 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1076)')))
```



---

### 目标

从

https://www.mh1234.com/comic/9329/559073.html

到

<https://www.mh1234.com/comic/9329/658969.html>

之间所有?p=2~p=9之间的图片

---

###### 爬取一张图片并保存

---

```python
#爬取一张图片，并保存
import requests
img_url = "https://mhpic.dongzaojiage.com/images/comic/330/658969/1564620602Tpjh4hhmdugtL_Xe.";
response = requests.get(img_url)
print(response.content)
with open('./commic/IBoy.jpg','wb') as f:
    f.write(response.content) 
```

###### 正则表达式

​	单纯的获取一张图片是没有问题的。但是如何获取那么多图片呢？

​	正则表达式是一种可以用来获取数据的方法。

---

###### 手动下载python第三方库文件

[python库文件](http://www.lfd.uci.edu/~gohlke/pythonlibs/))

Ctrl + F 搜索 "Regex, an alternative regular expression module, to replace re."

---

```python
import requests
import re
#发送一条get请求,并用response接受response返回的内容
commic_url='https://www.mh1234.com/comic/9329/559074.html?p=3';
response = requests.get(commic_url)
#print(response)
```

###### 爬虫和反爬虫

​	python的火爆使得很多网站开始提高安全意识，因此就有人反过来不让这些人爬取。那么这就叫做反爬虫。

###### 用户代理

---

User Agent中文名为用户代理，简称 UA，**它是一个特殊字符串头**，使得服务器能够识别客户使用的操作系统及版本、CPU 类型、浏览器及版本、浏览器渲染引擎、浏览器语言、浏览器插件等。***一些网站常常通过判断 UA 来给不同的操作系统、不同的浏览器发送不同的页面***，因此可能造成**某些页面无法在某个浏览器中正常显示**，但通过**伪装 UA 可以绕过检测**。浏览器向服务器发起请求的流程图，可以用下图表示：

![](https://mmbiz.qpic.cn/mmbiz_png/yjdDibu0qm7GT8FaV8PaacmYInklKtHI0v2V3P9KqDxKkVANGs43qoFL4If8AuJIbDfteEyn99ibrCkvxWtD3gVg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

---

Firefox 的 User-Agent：

```
Mozilla/5.0 (Macintosh; Intel Mac OS X 10.13; rv:63.0) Gecko/20100101 Firefox/63.0
```

Chrome 的 User-Agent：

```
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36
```

###### UA

- UA什么用

在网络请求当中，User-Agent 是标明身份的一种标识，服务器可以通过请求头参数中的 User-Agent 来判断请求方是否是浏览器、客户端程序或者其他的终端（当然，**User-Agent 的值为空也是允许的，因为它不是必要参数**）。

***可以说，UA是一种身份标记，没必要一定有***

- 为什么反爬虫会选择 User-Agent 这个参数呢？

从上面的介绍中，可以看出它是终端的身份标识。意味着服务器可以清楚的知道，这一次的请求是通过火狐浏览器发起的，还是通过 IE 浏览器发起的，甚至说是否是应用程序（比如 Python ）发起的。

网站的页面、动效和图片等内容的呈现是借助于浏览器的渲染功能实现的，浏览器是一个相对封闭的程序，因为它要确保数据的成功渲染，所以用户无法从浏览器中大规模的、自动化的获取内容数据。

而爬虫却不是这样的，爬虫生来就是为了获取网络上的内容并将其转化为数据。这是两种截然不同的方式，你也可以理解为通过编写代码来大规模的、自动化的获取内容数据，这是一种骚操作。

> 因为编程语言都有默认的标识，在发起网络请求的时候，这个标识在你毫不知情的情况下，作为请求头参数中的 User-Agent 值一并发送到服务器。比如 Python 语言通过代码发起网络请求时， User-Agent 的值中就包含 Python 。同样的，Java 和 PHP 这些语言也都有默认的标识。

- 

###### 反爬虫的黑名单策略

既然知道编程语言的这个特点，再结合实际的需求，那么反爬虫的思路就出来了。这是一中黑名单策略，只要出现在黑名单中的请求，都视为爬虫，对于此类请求可以不予处理或者返回相应的错误提示。

![](https://mmbiz.qpic.cn/mmbiz_png/yjdDibu0qm7GT8FaV8PaacmYInklKtHI0BZqQicFH7ztRjJQgZZ5iatFDT6HMOvRoiaafrKnwQVWZpr5g7PE6iaeulA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

---

###### 为什么用黑名单策略不用白名单策略？

> 因为浏览器太多

###### 通过 Nginx 服务日志来查看请求头中的 User-Agent

Nginx 是一款轻量级的 Web 服务器/反向代理服务器及电子邮件（IMAP/POP3）代理服务器。其特点是占有内存少，并发能力强，事实上 Nginx 的并发能力确实在同类型的网页服务器中表现较好，使用 Nginx 企业有：百度、京东、新浪、网易、腾讯、淘宝等。

###### Nginx 的安装与启动
通常可以使用系统本身的安装工具（Centos 的 yum、Debian 系的 apt-get 以及 MacOS 的 brew）安装 Nginx，以 linux 系统为例，在终端中输入：
> sudo apt-get install nginx

接下来根据提示选择，即可完成 Nginx 的安装。
接着在终端通过命令:
> sudo systemctl start nginx

即可启动 Nginx 服务。

> 备注：由于各个系统差别以及版本差异，安装和启动命令略有差别，解决办法自行搜索

###### Nginx 的日志

Nginx 为用户提供了日志功能，其中记录了每次服务器被请求的状态和其他信息，包括 User-Agent。 Nginx 的默认日志存放路径为：

/var/log/nginx/

在终端通过命令

> cd /var/log/nginx && ls

可以进入到日志存放目录并列出目录下的文件，可以看到其中有两个主要的文件，为 `access.log` 和 `error.log`

它们分别记录着成功的请求信息和错误信息。我们通过 Nginx 的访问日志来查看每次请求的信息。

###### Nginx 日志记录结果

上面使用了 4 种方法来向服务器发起请求，那么我们看看 Nginx 的日志中，记录了什么样的信息。在终端通过命令：

> sudo cat access.log

来查看日志文件。可以看到这几次的请求记录：

无论是 Python 还是 Curl 或者浏览器以及 Postman 的请求，都被记录在日志文件中，说明 Nginx 可以识别发起请求的终端类型。

###### 实现反爬虫

之前的理论和逻辑，在实验中都得到了验证，那么接下来我们就通过**黑名单策略**将 Python 和 Curl 发起的请求过滤掉，只允许 Firefox 和 Postman 的请求通过，并且对被过滤的请求返回 403 错误提示。

![](https://mmbiz.qpic.cn/mmbiz_png/yjdDibu0qm7GT8FaV8PaacmYInklKtHI0b8mZlUfVtkdvpX7EIqWB82E40icRf4qjWnGyMeWtLFxp1kqRslRrO0w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

反爬虫的过程如上图所示，相当于在服务器和资源之间建立了一道防火墙，在黑名单中的请求将会被当成垃圾丢弃掉。

###### 配置 Nginx 规则

Nginx 提供了配置文件以及对应的规则，允许我们过滤掉不允许通过的请求，本次反爬虫我们使用的就是它。Nginx 的配置文件通常放在`/etc/nginx/`目录下,名为`nginx.conf`，我们通过查看配置文件来看一看，站点的配置文件在什么地方。再通过系统自带的编辑器（笔者所用系统自带 Nano，其他系统可能自带 Vim）来编辑配置文件。在配置文件中找到站点配置文件地址（笔者所用电脑存放路径为`/etc/nginx/sites-enable`），再到站点配置文件中找到`local`级别的配置，并在其中加上一下内容：

> sudo nginx -s reload

整个操作过程如上图所示，让 Nginx 服务器重新载入配置文件，使得刚才的配置生效。

###### 反爬虫效果测试

重复上面访问的步骤，通过浏览器、Python 代码、Postman 工具和 Curl发起请求。从返回的结果就可以看到，与刚才是有所区别的。

- 浏览器返回的是正常的页面，说明没有收到影响；
- Python 代码的状态码变成了 403，而不是之前的 200
- Postman 跟之前一样，返回了正确的内容；
- Curl 跟 Python 一样，无法正确的访问资源，因为它们发起的请求都被过滤掉了。

**提示**：你可以继续修改 Nginx 的配置来进行测试，最终会发现结果会跟现在的一样：只要在黑名单中，请求就会被过滤掉并且返回 403 错误。

>  提示：这就是你平时编写爬虫代码时，需要在请求头中伪造浏览器的原因。

###### 绕过 User-Agent 方式的反爬虫

在 Requests 库中，允许用户自定义请求头信息，所以我们可以在请求头信息中将 User-Agent 的值改为浏览器的请求头标识，这样就能够欺骗 Nginx 服务器，达到绕过反爬虫的目的。将之前的 Python 代码改为：

```python
import requests
# 伪造请求头信息 欺骗服务器
headers = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.13; rv:9527.0) Gecko/20100101 Firefox/9527.0"}
resp = requests.get("http://127.0.0.1", headers=headers)
print(resp.status_code)
```

```python
import requests
# 伪造请求头信息 欺骗服务器
headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36"}
resp = requests.get("https://www.mh1234.com/comic/9329/559074.html?p=3", headers=headers)
print(resp.status_code)
```

###### python爬虫集中方法突破封锁的限制

1. UA 上文提到

   1. ```
      #简单表示只模拟User-Agent部分
      headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64)\
       AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'}
      然后用requests.get(url, headers = headers)进行请求，这种方法可以满足部门网页的请求，可以作为遇到问题的第一种尝试。
      
      ```

2. 携带cookies一些赋给Headers值，把Repuest Headers下的信息都构造成字典，赋给headers属性： 

   1. ```
      #复杂表示，因为爬取过程中发现只用User-Agent部分仍然会被防火墙阻挡
      request_headers = {
          'Accept':'text/html, */*; q=0.01',
          'Accept-Encoding':'gzip, deflate',
          'Accept-Language':'zh-CN,zh;q=0.9',
          'Connection':'keep-alive',
          'Cookie':'acw_tc=AQAAAAFSbEV6igQAU3joKmb1pHb7Amkd;_umdata=2BA477700510A7DF4059C61C78E8395939FAD577E94352328D751D6CD84A71DC483C7165B9EADDACCD43AD3E795C914C9155FE5460049F4934F91027D34B6E6F;PHPSESSID=lrcjhpkrjtbffirbtch8hrh9g2;hasShow=1;UM_distinctid=1616afff050228-00a2c5f077a432-3b60490d-144000-1616afff052387;CNZZDATA1254842228=1796631317-1517916172-%7C1517916172;_uab_collina=151791861517128939720221;zg_did=%7B%22did%22%3A%20%221616afff15d69f-06e4c98572085c-3b60490d-144000-1616afff15e48b%22%7D;zg_de1d1a35bfa24ce29bbf2c7eb17e6c4f=%7B%22sid%22%3A%201517918613857%2C%22updated%22%3A%201517918631284%2C%22info%22%3A%201517918613860%2C%22superProperty%22%3A%20%22%7B%7D%22%2C%22platform%22%3A%20%22%7B%7D%22%2C%22utm%22%3A%20%22%7B%7D%22%2C%22referrerDomain%22%3A%20%22%22%2C%22cuid%22%3A%20%228731ad9f297a7dbbcff0814343e06ea3%22%7D;',
          'Host':'www.qichacha.com',
          'Referer':'http://www.qichacha.com/firm_06396efe66551d4ac07ee8cb41b0e325.html',
          'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.119 Safari/537.36',
          'X-Requested-With':'XMLHttpRequest'
          }
      ```

   2. 这种携带cookie值得headers一般用于需要登录才能获取部分信息的网页。

3. 封锁间隔时间

```
通常对于某些网站，在我们发送请求后，因为访问速度过快，网页会很快发现我们的IP地址在一段时间内，多次像页面发送请求。对于一般的网站，它的反爬技术很可能就是利用公式计算某一IP地址在一段时间内发送请求的次数，因为一个正常人，在一定时间的请求都是有限的。 
所以，对于这种情况，我们可以简单的尝试在爬虫的过程中，对我们的程序进行适当的延时，调用time.sleep()函数。这样既不会过快的访问网页，对对方的服务器产生严重的垃圾和负担，也可以防止程序被迫中止。

举一个例子就是，当初我们尝试封锁IP破解的时候（接下来我说到），我找到了一个免费代理IP的地址，于是我就要想办法批量的把IP地址爬下来，然后在进行测试，因为免费的代理IP大都性能不好或者已经不能用了。这个时候当我第一次爬的时候，没有爬完一页程序就被迫中止了，（因为返回的页面内容变了），我就知道遇到了反爬，那么我首先尝试的就是利用时间延时，方法就是调用time.sleep()函数，并把延迟时间定为3s，没想到程序就顺利运行了，成功的爬了10页的代理IP地址。
所以在遇到反爬时，如果你觉得你访问的页面是一般网站，不是那种存着比较重要信息的网站，可以首先尝试利用时间延时。
```

``` python
import requests
# 伪造请求头信息 欺骗服务器
headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36"}
time.sleep(3)
resp = requests.get("https://www.mh1234.com/comic/9329/559074.html?p=3", headers=headers)
time.sleep(3)
print(resp.status_code)
```

4. 封锁ip限制

略

###### 问题明晰

通过报错信息知道是因为一个网络认证失败了，而这个网络认证是HTTPS才有的，因此HTTPS比HTTP安全些。

>  解决办法
>
>  1. 换用URLlib库
>
>  2. 还是使用requests库
>
>    1. 伪造SSL证书一并发送服务器继续伪装
>    2. 忽略报错继续访问获得资源
>
>  ```python
>  import requests
>  requests.packages.urllib3.disable_warnings()
>  
>  #requests.packages.urllib3.disable_warnings(InsecureRequestWarning)  # 或者这个
>  response = requests.get(url, verify=False)
>  ```

###### 最终成功

最全代码

```python
import time
import requests
# 伪造请求头信息 欺骗服务器
headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36"}
#忽略SSl证书
# requests.packages.urllib3.disable_warnings()
response = requests.get("https://www.mh1234.com/comic/9329/559074.html?p=3", verify=False)
# resp = requests.get(, headers=headers)
print(response.status_code)
```

最精代码

```python
import requests
#忽略SSl证书
response = requests.get("https://www.mh1234.com/comic/9329/559074.html?p=3", verify=False)
print(response.status_code)
```

有警告，但是成功打印

```python
Warning (from warnings module):
  File "C:\soft\python\lib\site-packages\urllib3\connectionpool.py", line 851
    InsecureRequestWarning)
InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
```

百度爬取代码

```python
import requests
import re
page_url = 'http://image.baidu.com/search/index?tn=baiduimage&ps=1&ct=201326592&lm=-1&cl=2&nc=1&ie=utf-8&word=%E9%83%AD%E6%99%B6%E6%99%B6'
response = requests.get(page_url, verify=False)
response.encoding='utf-8'
html=response.text
imgs=re.findall(r'"thumbURL":"(.*?)"',html)
print(imgs)

for index,img_url in enumerate(imgs):
	response = requests.get(img_url)
    with open('%s.%s'%(index,img_url.split('.')[-1]),'wb') as f:
        f.write(response.content)
```

###### 爬取网页上的图片的代码

> 使用说明：在本地新建一个不含中文的文件夹，将代码放入运行即可。
>
> url是根据index页面来找的

```python
import requests
import re
page_url = 'https://image.baidu.com/search/index?tn=baiduimage&ipn=r&ct=201326592&cl=2&lm=-1&st=-1&fm=result&fr=&sf=1&fmq=1565007452789_R&pv=&ic=&nc=1&z=&hd=&latest=&copyright=&se=1&showtab=0&fb=0&width=&height=&face=0&istype=2&ie=utf-8&sid=&word=%E7%8E%8B%E8%80%85%E8%8D%A3%E8%80%80&f=3&oq=wangzherongyao&rsp=1'
response = requests.get(page_url, verify=False)
response.encoding='utf-8'
html=response.text
imgs=re.findall(r'"thumbURL":"(.*?)"',html)
print(imgs)

for index,img_url in enumerate(imgs):

        
    response = requests.get(img_url)
    with open('%s.%s'%(index,img_url.split('.')[-1]),'wb') as f:
      f.write(response.content)
```

###### sublime的使用

因为要写python代码，python自带的不是很好用，因此学习使用submitText。

下载，安装。

在***使用教程***之前，还要自行联网安装Package Control

[使用教程](<https://www.cnblogs.com/zjiacun/p/9876806.html>)

---

###### 千辛万苦，终于成功

```python
# import requests 
# import re
# #设置url
# url='http://image.baidu.com/search/index?tn=baiduimage&ps=1&ct=201326592&lm=-1&cl=2&nc=1&ie=utf-8&word=%E7%99%BE%E9%87%8C%E5%AE%88%E7%BA%A6'
# #url = "https://mhpic.dongzaojiage.com/images/comic/280/559081/1562872247tkSWVuxQ5675SvHa."
# #处理https,获得get对象
# response = requests.get(url,verify=False)
# #处理乱码
# response.encoding='utf-8'
# #专门讲response赋值给html好后面操作
# html=response.text
# #开始写正则表达式过滤规则
# imgs=re.findall(r'"thumbURL":"(.*?)"',html)
# #用for循环保存到本地
# for index,img_url in enumerate(imgs):
#     response = requests.get(img_url)
#     with open('%s.%s'%(index,img_url.split('.')[-1]),'wb') as f:
#       f.write(response.content)


###### modified
import requests 
import re
#设置url
url = "https://www.mh1234.com//comic//9329//559080.html"
#处理https,获得get对象
response = requests.get(url,verify=False)
#处理乱码
response.encoding='utf-8'
#专门讲response赋值给html好后面操作
html=response.text
#开始写正则表达式过滤规则


#'https://mhpic.dongzaojiage.com/'+



#这个是chapterPath
chapterPath=re.findall(r'var chapterPath = "(.*)";var pageTitle',html)[0]

#这个text是chapterImages
chapterImages = re.findall(r'([1][5][6][2][8][7].*[.])["]+',html).pop(0).split('","')


#for index,img in enumerate(chapterImages):
#	with open('%s.%s'%(index,"txt"),'wb') as f:
#		f.write(bytes(img.encode('utf-8')))
# 现在，所有的bytes(img.encode('utf-8'))就都是合理的结果了
arraylist = list();
#通过遍历的形式组装数组
for index,img in enumerate(chapterImages):
	arraylist.append('https://mhpic.dongzaojiage.com/'+chapterPath+img)

for index,img_url in enumerate(arraylist):
 	response=requests.get(img_url)	
 	with open('%s.%s'%(index,"jpg"),'wb') as f:
	  f.write(response.content)

print('-------------------------------------------------------')
print(arraylist)
```

###### 完整版代码

```python
# import requests 
# import re
# #设置url
# url='http://image.baidu.com/search/index?tn=baiduimage&ps=1&ct=201326592&lm=-1&cl=2&nc=1&ie=utf-8&word=%E7%99%BE%E9%87%8C%E5%AE%88%E7%BA%A6'
# #url = "https://mhpic.dongzaojiage.com/images/comic/280/559081/1562872247tkSWVuxQ5675SvHa."
# #处理https,获得get对象
# response = requests.get(url,verify=False)
# #处理乱码
# response.encoding='utf-8'
# #专门讲response赋值给html好后面操作
# html=response.text
# #开始写正则表达式过滤规则
# imgs=re.findall(r'"thumbURL":"(.*?)"',html)
# #用for循环保存到本地
# for index,img_url in enumerate(imgs):
#     response = requests.get(img_url)
#     with open('%s.%s'%(index,img_url.split('.')[-1]),'wb') as f:
#       f.write(response.content)

###### modified
import requests 
import re
import os #创建文件夹所用
# #从文件读url
# with open('url.txt','wb') as f:
#   print(f.read())



#url生成
# 559073  ~   658969
for  x in range(254602,254756):
   
	url = 'https://www.mh1234.com/comic/11955/' + str(x) + '.html'
	#print(url)

	#url = "https://www.mh1234.com/comic/9329/558924.html"
	## Target  ::::::::::::::::::::::::::::::::::::::::::::::::559073  ~   658969
	#处理https,获得get对象
	#处理异常情况，如果连不上这个网站，那就跳过继续下一个
	try:
		response = requests.get(url,verify=False)
	except Exception as e:
		continue
	#处理乱码
	response.encoding='utf-8'
	#专门讲response赋值给html好后面操作
	html=response.text
	#开始写正则表达式过滤规则
    
    #睡3秒
    #time.sleep(3)


	#'https://mhpic.dongzaojiage.com/'+



	#这个是chapterPath
	chapterPath=re.findall(r'var chapterPath = "(.*)";var pageTitle',html)[0]


	#这个text是chapterImages
	try:
		chapterImages = re.findall(r'([1][5][6][2][8][7].*[.])["]+',html).pop(0).split('","')
	except Exception as e:
		x=x+1
		print(e)
		continue
	#for index,img in enumerate(chapterImages):
	#	with open('%s.%s'%(index,"txt"),'wb') as f:
	#		f.write(bytes(img.encode('utf-8')))
	# 现在，所有的bytes(img.encode('utf-8'))就都是合理的结果了
	arraylist = list();
	#通过遍历的形式组装数组
	for index,img in enumerate(chapterImages):
		arraylist.append('https://mhpic.dongzaojiage.com/'+chapterPath+img)

	#处理url，得到文件夹名
	dirname=url.split('/')[-1].split('.')[0]+"/"
	#创建目录
	os.mkdir(dirname)
	try:		
		for index,img_url in enumerate(arraylist):
		 	response=requests.get(img_url)	
		 	#将图片的二进制流写道里面
		 	with open(dirname+'%s.%s'%(index,"jpg"),'wb') as f:
			  f.write(response.content)
			  f.close()
	except Exception as e:
		print('!!!!!!!!!!!!!!!!!!!!!!!!!网站连接异常!!!!!!!!!!!!!!!!!!!!!!!!!!!')
		continue
```

